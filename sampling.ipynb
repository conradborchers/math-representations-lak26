{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e1d2f-008c-4bff-a8a6-54659cc511e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gc\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import pandas as pd\n",
    "import swifter \n",
    "\n",
    "from tqdm import tqdm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f1702-f8fc-4f73-9b4a-ff8613c96bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"QC23_Dataset.csv\"  # Path to your input CSV\n",
    "output_parquet_path = \"embedded_output.parquet\"  # Output file path\n",
    "chunk_size = 10000\n",
    "num_students = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e26d084-86c3-48cc-a879-b3cc85edd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return ''\n",
    "    s = s.lower() \n",
    "    s = re.sub(r'[\\\\s+]', ' ', s)\n",
    "    s = re.sub(r'[.,;!?]', '', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec605fdd-20fb-475b-be1a-413fc6d7ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L3-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33161ae8-62e2-45ee-a0b6-01653eba677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "collected_students = set()\n",
    "collecting = True\n",
    "all_chunks = []\n",
    "chunk_index = 0;\n",
    "no_data_chunk_counter = 0\n",
    "max_no_data_chunks = 1000;\n",
    "\n",
    "with tqdm(desc=\"Processing chunks\") as pbar:\n",
    "    for chunk in pd.read_csv(\n",
    "        file_path,\n",
    "        usecols=['student_user_id', 'problem_id', 'problem_type_id', 'skill_code', 'answer_text', 'correctness', 'problem_log_id', 'problem_start_time', 'problem_end_time', 'hint_count', 'problem_body', 'teacher_id', 'first_action_is_attempt'],\n",
    "        chunksize=chunk_size,\n",
    "        low_memory=True,\n",
    "        on_bad_lines='skip'\n",
    "    ):\n",
    "        try:\n",
    "            chunk = chunk.dropna(subset=['skill_code'])\n",
    "            chunk['ID'] = list(range(chunk_index * chunk_size, chunk_index * chunk_size + len(chunk)))\n",
    "            # Only collect new students if we haven't reached the target\n",
    "            if collecting:\n",
    "                new_students_mask = ~chunk['student_user_id'].isin(collected_students)\n",
    "                new_students = chunk.loc[new_students_mask, 'student_user_id'].unique()\n",
    "                needed = num_students - len(collected_students)\n",
    "                if needed > 0:\n",
    "                    selected_new = new_students[:needed]\n",
    "                    collected_students.update(selected_new)\n",
    "                if len(collected_students) >= num_students:\n",
    "                    collecting = False\n",
    "            \n",
    "            # Filter\n",
    "            chunk = chunk[chunk['student_user_id'].isin(collected_students)]\n",
    "            if chunk.empty:\n",
    "                no_data_chunk_counter += 1\n",
    "                if no_data_chunk_counter >= max_no_data_chunks:\n",
    "                    print(f\"No relevant data found in {max_no_data_chunks} consecutive chunks. Stopping early.\")\n",
    "                    break\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            # Preprocess\n",
    "            chunk['answer_text'] = chunk['answer_text'].swifter.apply(preprocessing_text)\n",
    "            unique_texts = chunk['answer_text'].drop_duplicates().tolist()\n",
    "    \n",
    "            # Embed\n",
    "            embeddings = model.encode(chunk['answer_text'].tolist(), show_progress_bar=True)\n",
    "            embeddings_body = model.encode(chunk['problem_body'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "            embedded_chunk = pd.DataFrame({\n",
    "                \"ID\": chunk['ID'].values,\n",
    "                \"problem_log_id\": chunk['problem_log_id'].values,\n",
    "                \"teacher_id\": chunk['teacher_id'].values,\n",
    "                \"problem_id\": chunk['problem_id'].values,\n",
    "                \"problem_type_id\": chunk['problem_type_id'].values,\n",
    "                \"first_action_is_attempt\": chunk['first_action_is_attempt'].values,\n",
    "                \"hint_count\": chunk['hint_count'].values,\n",
    "                \"answer_text\": chunk['answer_text'].values,\n",
    "                \"problem_body\": chunk['problem_body'].values,\n",
    "                \"student_user_id\": chunk['student_user_id'].values,\n",
    "                \"skill_code\": chunk['skill_code'].values,\n",
    "                \"correctness\": chunk['correctness'].values,\n",
    "                \"problem_start_time\": chunk['problem_start_time'].values,\n",
    "                \"problem_end_time\": chunk['problem_end_time'].values,\n",
    "                \"embedding_response\": list(embeddings),\n",
    "                \"embedding_problembody\": list(embeddings_body),\n",
    "            })\n",
    "    \n",
    "            all_chunks.append(embedded_chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nSkipping chunk {chunk_index} due to error: {str(e)}\")\n",
    "        del chunk, embeddings, embedded_chunk\n",
    "        gc.collect()\n",
    "        chunk_index += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "final_df.to_parquet(\n",
    "    output_parquet_path,\n",
    "    index=False,\n",
    "    engine='pyarrow',\n",
    "    compression='snappy'\n",
    ")\n",
    "\n",
    "final_df = pd.concat(all_chunks, ignore_index=True)\n",
    "final_df.drop(columns=['answer_text', 'problem_body']).to_parquet(\n",
    "    output_parquet_path.replace('.parquet', '--no-text.parquet'),\n",
    "    index=False,\n",
    "    engine='pyarrow',\n",
    "    compression='snappy'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
